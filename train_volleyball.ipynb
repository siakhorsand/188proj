{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c5e6af",
   "metadata": {},
   "source": [
    "# Slime Volleyball Agent Training\n",
    "This notebook implements training for the Slime Volleyball environment using the Cross Entropy Method (CEM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3013b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlimeVolley-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Import our training modules\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcem_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CEMAgent\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_cem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_cem\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "from matplotlib import animation\n",
    "import gym\n",
    "import os\n",
    "\n",
    "# Monkey patch the gym registration function to handle duplicate registrations\n",
    "original_register = gym.envs.registration.register\n",
    "\n",
    "def patched_register(id, **kwargs):\n",
    "    if id in gym.envs.registry.env_specs:\n",
    "        return\n",
    "    return original_register(id, **kwargs)\n",
    "\n",
    "# Apply the patch\n",
    "gym.envs.registration.register = patched_register\n",
    "\n",
    "# Now import slimevolleygym\n",
    "import slimevolleygym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('SlimeVolley-v0')\n",
    "\n",
    "# Import our training modules\n",
    "from models.cem_model import CEMAgent\n",
    "from training.train_cem import train_cem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device for PyTorch\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Apple Silicon GPU)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4827119",
   "metadata": {},
   "source": [
    "## Environment Information\n",
    "The Slime Volleyball environment provides:\n",
    "- Observation space: 12-dimensional state vector containing:\n",
    "  - Agent position (x, y) and velocity (vx, vy)\n",
    "  - Ball position (x, y) and velocity (vx, vy)\n",
    "  - Opponent position (x, y) and velocity (vx, vy)\n",
    "- Action space: 3 binary actions [forward, backward, jump]\n",
    "- Reward: +1 for scoring, -1 for being scored on\n",
    "- Episode ends when one player scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a62a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print environment details\n",
    "print(\"Environment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "\n",
    "# Initialize the CEM agent with correct parameters\n",
    "agent = CEMAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=3,  # [forward, backward, jump]\n",
    "    population_size=50,  # Size of the population for CEM\n",
    "    elite_ratio=0.2,    # Top 20% are elite\n",
    "    noise_std=0.1,      # Standard deviation of noise\n",
    "    learning_rate=0.01  # Learning rate for parameter updates\n",
    ")\n",
    "\n",
    "print(\"\\nAgent Architecture:\")\n",
    "print(f\"Input dimensions: {env.observation_space.shape[0]}\")\n",
    "print(f\"Output dimensions: {env.action_space.n}\")\n",
    "print(f\"Population size: {agent.population_size}\")\n",
    "print(f\"Elite ratio: {agent.elite_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd396f",
   "metadata": {},
   "source": [
    "## Environment Test\n",
    "Let's test the environment by running a few random actions and displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18349faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment(env, num_steps=300):  # 10 seconds at 30 FPS\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    print(\"Running environment test...\")\n",
    "    for step in range(num_steps):\n",
    "        # Random action: one hot vector\n",
    "        action = np.zeros(3)\n",
    "        action[np.random.randint(3)] = 1  # Set one action to 1\n",
    "        \n",
    "        # Step environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Render and capture frame\n",
    "        try:\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            frames.append(frame)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not render frame: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode finished after {step} steps with reward {total_reward}\")\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "    \n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "# Run test and create animation\n",
    "try:\n",
    "    frames = test_environment(env)\n",
    "    \n",
    "    if frames:\n",
    "        # Create animation\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        patch = plt.imshow(frames[0])\n",
    "        plt.axis('off')\n",
    "        plt.title('Slime Volleyball Environment Test')\n",
    "\n",
    "        def animate(i):\n",
    "            patch.set_array(frames[i])\n",
    "            return [patch]\n",
    "\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig, animate, frames=len(frames),\n",
    "            interval=1000/30,  # 30 FPS\n",
    "            blit=True\n",
    "        )\n",
    "\n",
    "        plt.close()\n",
    "        display(HTML(anim.to_jshtml()))\n",
    "    else:\n",
    "        print(\"No frames were captured. Please check if XQuartz is running and properly configured.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during environment test: {e}\")\n",
    "    print(\"Please make sure XQuartz is running and properly configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbfc23",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "Set up the hyperparameters for training the agent using CEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "hyperparameters = {\n",
    "    'num_episodes': 100,      # Total episodes to train\n",
    "    'batch_size': 16,         # Episodes per batch\n",
    "    'elite_frac': 0.2,        # Top fraction of episodes to use for update\n",
    "    'eval_interval': 10,      # Episodes between evaluations\n",
    "    'save_interval': 20,      # Episodes between model saves\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2a0cd",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Train the agent using the Cross Entropy Method:\n",
    "1. Run multiple episodes with the current policy\n",
    "2. Select the top performing episodes\n",
    "3. Update the policy using the elite episodes\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory if it doesn't exist\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "metrics = train_cem(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    device=device,\n",
    "    num_episodes=hyperparameters['num_episodes'],\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    elite_frac=hyperparameters['elite_frac'],\n",
    "    eval_interval=hyperparameters['eval_interval'],\n",
    "    save_interval=hyperparameters['save_interval'],\n",
    "    save_dir=save_dir\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fb51b",
   "metadata": {},
   "source": [
    "## Training Visualization\n",
    "Plot the training progress and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343445a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics['episode_rewards'], 'b-', alpha=0.3, label='Episode Reward')\n",
    "plt.plot(metrics['mean_rewards'], 'r-', label='Moving Average')\n",
    "plt.fill_between(\n",
    "    range(len(metrics['mean_rewards'])),\n",
    "    np.array(metrics['mean_rewards']) - np.array(metrics['std_rewards']),\n",
    "    np.array(metrics['mean_rewards']) + np.array(metrics['std_rewards']),\n",
    "    alpha=0.2, color='r'\n",
    ")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot win rate\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 10\n",
    "wins = [r > 0 for r in metrics['episode_rewards']]\n",
    "win_rate = [sum(wins[max(0, i-window):i])/min(i, window) \n",
    "            for i in range(1, len(wins)+1)]\n",
    "plt.plot(win_rate, 'g-', label='Win Rate')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title(f'Win Rate (Moving Average, Window={window})')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8ce21",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "Save the training metrics and configuration for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017dae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results = {\n",
    "    'metrics': metrics,\n",
    "    'hyperparameters': hyperparameters,\n",
    "    'final_mean_reward': float(np.mean(metrics['episode_rewards'][-10:])),\n",
    "    'best_reward': float(max(metrics['episode_rewards'])),\n",
    "    'final_win_rate': win_rate[-1],\n",
    "    'training_duration': metrics.get('training_duration', 0)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_file = f'results/training_results_{timestamp}.json'\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Final average reward (last 10 episodes): {results['final_mean_reward']:.2f}\")\n",
    "print(f\"Best episode reward: {results['best_reward']:.2f}\")\n",
    "print(f\"Final win rate: {results['final_win_rate']:.2%}\")\n",
    "print(f\"Training duration: {results['training_duration']:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
